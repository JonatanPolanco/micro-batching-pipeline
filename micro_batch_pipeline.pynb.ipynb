{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción:\n",
    "\n",
    "###### Este notebook implementa un pipeline de microbatching con PySpark y PostgreSQL procesando los archivos CSV en lotes pequeños (escalados automaticamente). Ademas, almacena los datos en una base de datos y calcula estadísticas de manera incrementaltransaction_statistics.\n",
    "##### Para este pipeline usamos buenas practicas de desarrollo las cuales listamos a continuación:\n",
    "###### 1. manejo de variables de entorno\n",
    "###### 2. definimos esquema fijo en pyspark para mejorar el rendimiento\n",
    "###### 3. implementamos notacion snake_case y evitamos usar palabras reservadas como nombres de campos.\n",
    "###### 4. implementamos validacion de datos para asegurar la calidad de los mismos\n",
    "###### 5. realizamos calculos incrementales por cada micro batch para asegurar la eficiencia en el procesamiento y no hacer calculos sobre todos los datos.\n",
    "###### 6. hacemos buen manejo de los logs usando la librería Logging\n",
    "###### 7. manejo de errores en el flujo (try except) y para lectura/escritura en postgresql\n",
    "###### 8. Lo hacemos escalable automaticamente. No usamos un tabaño de batch fijo pensando en que este script funcione para datasets con gran volumen de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paso a paso del pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### importamos librerias y configuracion Logging, sesion de Pyspark y definimos el esquema fijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, expr, to_date, avg, min as pyspark_min, max as pyspark_max\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import logging\n",
    "\n",
    "\n",
    "# Configurar PySpark para que use \"python\" en lugar de \"python3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\")\n",
    "}\n",
    "\n",
    "JDBC_URL = f\"jdbc:postgresql://{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "JDBC_PROPERTIES = {\n",
    "    \"user\": DB_CONFIG[\"user\"],\n",
    "    \"password\": DB_CONFIG[\"password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Esquema definido para mejorar rendimiento\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"price\", FloatType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Crea una sesión de Spark con soporte para PostgreSQL.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Microbatching Pipeline\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"jars/postgresql-42.7.5.jar\") \\\n",
    "        .config(\"spark.jars\", \"jars/postgresql-42.7.5.jar\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 8) \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creamos funcion para leer los csv usando la funcion read.csv de Pyspark. \n",
    "#### convertimos el campo de fecha a timestamp ya que lo leemos inicialmente como string. canbiamos nombre de campo de fecha a transaction_date con el fin de no usar la palabra reservada \"timestamp\" y evitar errores de lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path, spark):\n",
    "    \"\"\"Carga un archivo CSV y convierte el campo de fecha correctamente.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Leyendo archivo: {file_path}\")\n",
    "        \n",
    "        df = spark.read.csv(file_path, header=True, schema=SCHEMA)\n",
    "        \n",
    "        # Convertimos 'timestamp' a 'transaction_date' con el formato correcto\n",
    "        df = df.withColumn(\"transaction_date\", to_date(col(\"timestamp\"), \"M/d/yyyy\")) \\\n",
    "               .drop(\"timestamp\")  # Eliminamos la columna original\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al leer {file_path}: {str(e)}\", exc_info=True)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implementamos validacion de datos:\n",
    "- el precio debe ser positivo\n",
    "- los campos clave de la tabla son user-id y transaction_date por los cuales no deben estar en nulo\n",
    "- si existe algun error, lo mostramos en consola y lo almacenamos en un dataframe para poder tener visibilidad de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"Valida los datos y reporta casuísticas de errores.\"\"\"\n",
    "\n",
    "    # Definir condiciones de error\n",
    "    df = df.withColumn(\"error_type\", expr(\"\"\"\n",
    "    CASE \n",
    "        WHEN price IS NULL THEN 'price_null'\n",
    "        WHEN price <= 0 THEN 'price_negative_or_zero'\n",
    "        WHEN transaction_date IS NULL THEN 'date_null'\n",
    "        WHEN user_id IS NULL THEN 'user_id_null'\n",
    "        ELSE NULL \n",
    "    END\n",
    "\"\"\"))\n",
    "\n",
    "    # Filtrar registros válidos e inválidos\n",
    "    df_valid = df.filter(col(\"error_type\").isNull()).drop(\"error_type\")\n",
    "    df_invalid = df.filter(col(\"error_type\").isNotNull())\n",
    "\n",
    "    # Contar tipos de errores\n",
    "    error_counts = df_invalid.groupBy(\"error_type\").agg(count(\"*\").alias(\"count\")).collect()\n",
    "\n",
    "    if error_counts:\n",
    "        error_message = \"Se detectaron registros inválidos con las siguientes casuísticas:\\n\"\n",
    "        for row in error_counts:\n",
    "            error_message += f\"- {row['error_type']}: {row['count']} registros.\\n\"\n",
    "        logger.warning(error_message.strip())\n",
    "\n",
    "    return df_valid, df_invalid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo incremental de estadísticas\n",
    "Para evitar recalcular todo en cada batch, usamos la siguiente estrategia:\n",
    "- creamos tabla `transaction_statistics` en PostgreSQL.\n",
    "- Obtenemos los valores acumulados (`total_count`, `avg_price`, `min_price`, `max_price`) de PostgreSQL. Dado en caso de que sea la primera iteración, inicializamos los valores.\n",
    "- Calculamos las estadísticas del batch actual.\n",
    "- Combinamos los valores previos con los nuevos usando un **promedio ponderado**.\n",
    "- Actualizamos la tabla `transaction_statistics` en PostgreSQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculo del batch actual:\n",
    "- Utilizamos funciones de agregación (`count`, `avg`, `min`, `max`) en PySpark.\n",
    "- Extraemos los valores generados y los almacenamos en variables.\n",
    "- Evitamos errores asegurando que los cálculos solo se realicen sobre registros válidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_batch_statistics(df):\n",
    "    \"\"\"Calcula estadísticas del batch actual.\"\"\"\n",
    "    stats = df.select(\n",
    "        count(\"*\").alias(\"batch_count\"),\n",
    "        avg(\"price\").alias(\"batch_avg\"),\n",
    "        pyspark_min(\"price\").alias(\"batch_min\"),\n",
    "        pyspark_max(\"price\").alias(\"batch_max\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    return stats.batch_count, stats.batch_avg, stats.batch_min, stats.batch_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculo del batch anterior. para esto hacemos lo siguiente:\n",
    "- Obtenemos el registro más reciente de la tabla  `transaction_statistics` ordenado por `total_count DESC`.  con esto, obtenemos el valor mas reciente.\n",
    "- Si no hay registros en la tabla, inicializamos los valores con `0` o `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_previous_statistics(spark):\n",
    "    \"\"\"Obtiene las estadísticas acumuladas desde PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        query = \"(SELECT * FROM transaction_statistics ORDER BY total_count DESC LIMIT 1) AS stats\"\n",
    "        stats_df = spark.read.jdbc(url=JDBC_URL, table=query, properties=JDBC_PROPERTIES)\n",
    "\n",
    "        if stats_df.count() == 0:\n",
    "            logger.warning(\"La tabla transaction_statistics está vacía. Inicializando valores predeterminados.\")\n",
    "            return 0, 0.0, None, None  # Inicializa valores si no hay datos\n",
    "\n",
    "        stats = stats_df.collect()[0]  # Extraer la primera fila\n",
    "\n",
    "        return (\n",
    "            int(stats[\"total_count\"]),\n",
    "            float(stats[\"avg_price\"]),\n",
    "            float(stats[\"min_price\"]) if stats[\"min_price\"] is not None else None,\n",
    "            float(stats[\"max_price\"]) if stats[\"max_price\"] is not None else None\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al leer estadísticas previas: {str(e)}\", exc_info=True)\n",
    "        return 0, 0.0, None, None  # Retorna valores predeterminados en caso de error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### realizamos el calculo incremental:\n",
    "- implementamos calculo de promedio en forma de ponderado (teniendo en cuenta el resultado del batch anterior): ((prev_avg * prev_count) + (batch_avg * batch_count)) / (prev_count + batch_count)\n",
    "- para el minimo, maximo: obtenemos los valores máximos entre los calculados en el batch actual y los calculados en el bathc anterior.\n",
    "- para el conteo: sumanos los registros del batch anterior con los del batch actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import builtins  # Importamos la versión nativa de min y max\n",
    "\n",
    "def update_statistics_in_postgres(batch_count, batch_avg, batch_min, batch_max, spark):\n",
    "    \"\"\"Actualiza las estadísticas acumuladas en PostgreSQL.\"\"\"\n",
    "\n",
    "    prev_count, prev_avg, prev_min, prev_max = get_previous_statistics(spark)\n",
    "\n",
    "    # Calcular el nuevo promedio ponderado\n",
    "    if prev_count == 0:\n",
    "        new_avg = batch_avg  # Primer batch\n",
    "    else:\n",
    "        new_avg = ((prev_avg * prev_count) + (batch_avg * batch_count)) / (prev_count + batch_count)\n",
    "\n",
    "    # Determinar nuevos min y max evitando errores con None\n",
    "    min_values = [x for x in [prev_min, batch_min] if x is not None]\n",
    "    new_min = builtins.min(min_values) if min_values else None\n",
    "\n",
    "    max_values = [x for x in [prev_max, batch_max] if x is not None]\n",
    "    new_max = builtins.max(max_values) if max_values else None\n",
    "\n",
    "    new_count = prev_count + batch_count\n",
    "\n",
    "    logger.info(f\"Actualizando estadísticas -> Count: {new_count}, Avg: {new_avg:.2f}, Min: {new_min}, Max: {new_max}\")\n",
    "\n",
    "    try:\n",
    "        # Crear un DataFrame con los nuevos valores\n",
    "        stats_df = spark.createDataFrame([\n",
    "            (new_count, new_avg, new_min, new_max)\n",
    "        ], [\"total_count\", \"avg_price\", \"min_price\", \"max_price\"])\n",
    "\n",
    "        # Sobreescribir la tabla con los nuevos valores\n",
    "        stats_df.write.jdbc(url=JDBC_URL, table=\"transaction_statistics\", mode=\"overwrite\", properties=JDBC_PROPERTIES)\n",
    "        logger.info(\"Estadísticas actualizadas correctamente en PostgreSQL.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al actualizar estadísticas en PostgreSQL: {str(e)}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardamos los datos en PostgreSQL\n",
    "\n",
    "Después de validar los datos y calcular las estadísticas del batch, guardamos los registros válidos en la base de datos PostgreSQL usando JDBC\n",
    "\n",
    "   - Contamos cuántas filas serán insertadas para monitorear la cantidad de datos procesados.\n",
    "\n",
    "   - Especificamos el modo `\"append\"` para agregar nuevos registros sin sobrescribir los existentes.\n",
    "\n",
    "   - Si ocurre un error al escribir en la base de datos, se registra en el log sin detener el pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_postgres(df, table_name):\n",
    "    \"\"\"Guarda los datos en PostgreSQL usando JDBC.\"\"\"\n",
    "\n",
    "    # Verificar cantidad de registros antes de escribir\n",
    "    row_count = df.count()\n",
    "    logger.info(f\"Se encontraron {row_count} registros para insertar en {table_name}\")\n",
    "\n",
    "    try:\n",
    "        df.write.jdbc(url=JDBC_URL, table=table_name, mode=\"append\", properties=JDBC_PROPERTIES)\n",
    "        logger.info(f\"Datos insertados en la tabla {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al guardar en PostgreSQL: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procesamos un archivo CSV (`process_csv`)\n",
    "\n",
    "Esta función ejecuta el flujo completo del pipeline para un archivo CSV, asegurando que cada batch sea validado, insertado en PostgreSQL y utilizado para actualizar las estadísticas acumuladas. Ejecuta las funcionesen el orden: read_csv, validate_data, compute_batch_statistics y update_statistics_in_postgres\n",
    "\n",
    "   - Leemos el archivo con el esquema definido en **PySpark**.\n",
    "   - Convertimos el campo `timestamp` a `transaction_date`.\n",
    "   - Filtramos registros inválidos (precios negativos, fechas nulas, etc.).\n",
    "   - Separa registros válidos e inválidos para evitar insertar información incorrecta.\n",
    "   - Insertamos solo los datos **válidos** en la tabla `transactions`.\n",
    "   - Obtenemos `batch_count`, `batch_avg`, `batch_min` y `batch_max` sobre los datos procesados.\n",
    "   - Consultamos las estadísticas previas desde `transaction_statistics`.\n",
    "   - Aplicamos un **cálculo incremental** para actualizar las métricas.\n",
    "   - Guardamos la nueva versión de las estadísticas en PostgreSQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv(file_path, spark):\n",
    "    \"\"\"Pipeline de microbatch para un archivo CSV con estadísticas.\"\"\"\n",
    "    df = read_csv(file_path, spark)\n",
    "    if df is None or df.limit(1).count() == 0:\n",
    "        logger.warning(f\"Archivo {file_path} vacío o con errores, omitiendo.\")\n",
    "        return\n",
    "\n",
    "    df_valid, df_invalid = validate_data(df)\n",
    "\n",
    "    save_to_postgres(df_valid, \"transactions\")\n",
    "\n",
    "    # Calcular estadísticas del batch actual\n",
    "    batch_count, batch_avg, batch_min, batch_max = compute_batch_statistics(df_valid)\n",
    "\n",
    "    # Actualizar estadísticas en PostgreSQL\n",
    "    update_statistics_in_postgres(batch_count, batch_avg, batch_min, batch_max, spark)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La función `main()` orquesta la ejecución del **pipeline de microbatching**, procesando múltiples archivos CSV en secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Ejecuta el pipeline de microbatching.\"\"\"\n",
    "    spark = create_spark_session()\n",
    "    file_paths = [\"data/2012-1.csv\", \"data/2012-2.csv\", \"data/2012-3.csv\", \"data/2012-4.csv\", \"data/2012-5.csv\"]\n",
    "\n",
    "    for file in file_paths:\n",
    "        process_csv(file, spark)\n",
    "\n",
    "    logger.info(\"Pipeline completado.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### para seleccionar las estadísticas completas del pipeline, podemos hacer\n",
    "#### select * de la tabla 'transaction_statistics' lo que devolverá una sola linea con las estadisticas calculadas.\n",
    "\n",
    "#### en los logs estamos mostrando las estadisticas de cada batch, con el fin de facilitar el seguimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
